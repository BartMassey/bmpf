\documentclass[12pt]{article}
\usepackage{bigpage}
\usepackage{blockpar}

\newcommand{\asgn}{\,\,\leftarrow\,\,}
\newcommand{\newcode}{\\*[0.5\baselineskip]}

\title{Linear Time Weighted Resampling}
\author{Bart Massey}
\date{17 August 2007}

\begin{document}
  \maketitle

  \begin{abstract}
  We describe an optimal algorithm for perfect weighted resampling
  of a population, with time complexity $O(m + n)$ for
  resampling $m$ inputs to produce $n$ outputs.  This
  is an advance over the perfect $O(m + n \log m)$ algorithm and
  approximate linear-time algorithm that represent the
  state of the art.  Our resampling algorithm is also easily
  parallelizable, with linear speedup.
  Linear-time resampling yields
  substantial improvements in our motivating example of
  Bayesian Particle Filtering.
  \end{abstract}

\section{Introduction}

  Bayesian Particle Filtering (BPF) is an exciting new methodology
  for state space tracking and sensor fusion.  The bottleneck
  step in BPF is weighted resampling: creating a new
  population of ``particles'' from an old population by
  random sampling from the source population according to
  the particle weights.    Consider resampling $m$ inputs to
  produce $n$ outputs.  A standard na\"ive algorithm has time complexity
  $O(mn)$. Algorithms with complexity $O(m + n \log m)$ are easy
  to find, although they seem not to be widely used in
  practice.  Instead, typical BPF implementations will resample
  using the expensive $O(mn)$ algorithm, but only
  sporadically.  A more recent approach is to use a linear
  time algorithm, but one that produces a sample that is
  only approximately correctly weighted~\ref{?}.

  We introduce an algorithm with time complexity $O(m + n)$
  (requiring just the normal $O(m + n)$ space) that produces
  a perfectly weighted sample. For a simple array
  representation of the output, simply initializing the
  output will require time $O(n)$.  It seems that the input
  must be scanned at least once just to determine the total
  input weight for normalization.  Thus, our algorithm is
  apparently optimal.

  BPF is typically computation-limited, and all of the other
  steps in a BPF iteration require time linear in the
  population size.  By linearizing resampling, we remove the
  resampling bottleneck, allowing much higher population
  sizes that in turn dramatically improve BPF performance.

  A good starting point for describing our algorithm is to
  review the standard resampling algorithms.  We then
  describe our algorithm and report its effectiveness in a
  BPF implementation.

\subsection{An $O(mn)$ Resampling Algorithm}

  The na\"ive approach to resampling has been re-invented
  many times.  Assume an array $s$ of $m$ input samples, and an
  output array $s'$ that will hold the $n$ output samples of
  the resampling.  Assume
  further that associated with each sample $s_i$ is a
  weight $w(s_i)$, and that the weights have been normalized
  to sum to 1 (which can of course be done in linear time).
  In this setting a correct, if inefficient, way to resample is via the
  pseudocode of Figure~\ref{fig-omn}.  The {\em sample}
  procedure selects the first sample such that the sum of
  weights in the input up to and including this sample is
  greater than some index value $\mu$.  The index value is chosen
  in {\em resample} by uniform random sampling from the distribution
  $[0..1]$, with each output position being filled in
  turn.

  \begin{figure}
    \centering
    \begin{minipage}{0.6\textwidth}
      \begin{tabbing}
      XX\=XXXX\=XXXX\=XXXX\=\kill
      {\bf to} {\it sample}$(\mu)$: \\
      \>$t \asgn 0$\\
      \>{\bf for} $i$ {\bf from} $1$ {\bf to} $m$\\
      \>\>$t \asgn t + w(s_i)$\\
      \>\>{\bf if} $t > \mu$ {\bf then}\\
      \>\>\>{\bf return} $s_i$\newcode
      {\bf to} {\it resample}: \\
      \>{\bf for} $i$ {\bf from} $1$ {\bf to} $n$\\
      \>\>$\mu \asgn \textit{random-real}([0..1])$\\
      \>\>$s'_i \asgn sample(\mu)$
      \end{tabbing}
    \end{minipage}
    \caption{Na\"ive Resampling}\label{fig-omn}
  \end{figure}

  The na\"ive algorithm has its advantages.  It is easy to
  verify that it is a perfect sampling algorithm.  It is
  easy to implement, and easy to parallelize.  While
  expected running time is $o(\frac{1}{2}mn)$, the
  proportionality constant can be improved substantially by
  paying $O(m \log m)$ time up front to sort the input array
  so that the largest weights occur first.  That said, this
  is still the bottleneck step in a typical BPF
  implementation.

\subsection{An $O(m + n \log m)$ Resampling Algorithm}

  One simple way to improve the performance of the na\"ive
  algorithm is to improve upon the linear scan performed by
  {\em sample} in Figure~\ref{fig-omn}.

  For example, imagine that the input array is treated as a
  binary heap.  In time $O(m)$ we can compute and cache the
  sum $w_l$ of weights of the left sub-heap of each position
  in the input, as shown in Figure~\ref{fig-heap}.  First,
  the sum at each heap position is computed bottom-up and
  stored as $w_t$.  This then gives $w_l$ as simply the
  $w_t$ of the left child.

  \begin{figure}
    \centering
    \begin{minipage}{0.6\textwidth}
      \begin{tabbing}
      XX\=XXXX\=XXXX\=XXXX\=\kill
      {\bf to} {\it init-weights}: \\
      \>{\bf for} $i$ {\bf from} $m$ {\bf downto} $1$\\
      \>\>{\bf if} $2i > m$ {\bf then}\\
      \>\>\>$w_t(s_i) \asgn w(s_i)$\\
      \>\>{\bf else if} $2i + 1 > m$ {\bf then}\\
      \>\>\>$w_t(s_i) \asgn w_t(s_{2i}) + w(s_i)$\\
      \>\>{\bf else}\\
      \>\>\>$w_t(s_i) \asgn w_t(s_{2i}) + w(s_i) + w_t(s_{2i+1})$\\
      \>{\bf for} $i$ {\bf from} $1$ {\bf to} $m$\\
      \>\>{\bf if} $2i > m$ {\bf then}\\
      \>\>\>$w_l(s_i) \asgn 0$\\
      \>\>{\bf else}\\
      \>\>\>$w_l(s_i) \asgn w_t(s_{2i})$
    \end{tabbing}
    \end{minipage}
    \caption{Computing Weights of Left Sub-heaps}\label{fig-heap}
  \end{figure}

  Given $w_l$, {\em sample} can perform a scan for the
  correct input weight in time $O(\log m)$ by scanning down
  from the top of the heap, as shown in
  Figure~\ref{fig-onlm}.  At each step, if the target
  variate $\mu$ is less than the weight of the left subtree,
  the scan descends left.  If $\mu$ is greater than the
  weight of the left subtree by less than the weight of the
  current node the scan terminates and this node is
  selected.  Otherwise, the scan descends right.

  \begin{figure}
    \centering
    \begin{minipage}{0.6\textwidth}
      \begin{tabbing}
      XX\=XXXX\=XXXX\=XXXX\=\kill
      {\bf to} {\it sample}$(\mu,i)$: \\
      \>{\bf if} $\mu \le w_l(s_i)$ {\bf then} \\
      \>\>{\bf return} {\it sample}$(\mu,2i)$\\
      \>{\bf if} $\mu \le w_l(s_i) + w(s_i)$ {\bf then} \\
      \>\>{\bf return} $s_i$\\
      \>{\bf return} {\it sample}$(\mu,2i + 1)$
    \end{tabbing}
    \end{minipage}
    \caption{Faster Sampling}\label{fig-onlm}
  \end{figure}

  This algorithm is a bit more complex than the na\"ive one,
  but it dramatically improves upon the worst-case running
  time.  As with the na\"ive algorithm, good constant-factor
  improvements are possible by actually heapifying the input
  such that the largest-weighted inputs are near the top.
  Heapification is also $O(m)$ time and can be done in
  place, so there is no computational complexity penalty for
  this optimization.

\subsection{An $O(n log n + m)$ Resampling Algorithm}

  One can imagine trying to improve upon the complexity of
  the heap-style algorithm by using some more efficient data
  structure.  However, there is a fundamental tradeoff---the
  setup for an improved algorithm needs to continue to have
  a cost low in $m$.  Otherwise, any savings in resampling
  will be swamped by setup in the common case that $m
  \approx n$.

  A better plan is to try to improve the na\"ive algorithm
  in a different way.  The real problem with the na\"ive
  algorithm is not so much the cost per scan as it is the
  fact that each scan is independent.  It seems a shame not
  to reuse the work of the initial scan in subsequent scans.

  Imagine, for a moment, that an array $u$ of $n$ variates
  was generated up-front, then sorted.  At this point, a
  {\em merge} operation, as shown in Figure~\ref{fig-merge},
  could be used to generate all $n$ outputs in a single pass
  over the $m$ inputs.


  \begin{figure}
    \centering
    \begin{minipage}{0.6\textwidth}
      \begin{tabbing}
      XX\=XXXX\=XXXX\=XXXX\=\kill
      {\bf to} {\it merge}$({\bf u})$: \\
      \>$j \asgn 1$ \\
      \>$t \asgn 0$ \\
      \>$\mu \asgn u_1$ \\
      \>{\bf for} $i$ {\bf from} $1$ {\bf to} $n$\\
      \>\>{\bf while} $\mu < t$ {\bf do}\\
      \>\>\>$t \asgn t + w(s_j)$\\
      \>\>\>$j \asgn j + 1$\\
      \>\>$s'_i \asgn s_j
    \end{tabbing}
    \end{minipage}
    \caption{Faster Sampling}\label{fig-merge}
  \end{figure}

\end{document}
