\documentclass[12pt]{article}
\usepackage{bigpage}
\usepackage{blockpar}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amscd}
\usepackage{url}

\newcommand{\asgn}{\,\,\leftarrow\,\,}
\newcommand{\newcode}{\\*[0.5\baselineskip]}
\newcommand{\Prob}{\text{Pr}}
\newcommand{\hyperg}{{{{}_2}{F_1}}}
\newcommand{\du}{{\text{d}u}}

\title{Fast Perfect Weighted Resampling}
\author{Bart Massey\\
  Assoc. Prof. Computer Science\\
  Portland State University\\
  \url{bart@cs.pdx.edu}}
\date{Draft of \today\\{\em Do Not Distribute}}

\begin{document}
  \maketitle

  \begin{abstract}
  We describe an algorithm for perfect weighted-random
  resampling of a population with time complexity $O(m +
  n)$ for resampling $m$ inputs to produce $n$ outputs.
  This algorithm is an incremental improvement over
  standard resampling algorithms.  Our resampling
  algorithm is parallelizable, with linear speedup.
  Linear-time resampling yields notable performance
  improvements in our motivating example of Sequential
  Importance Resampling for Bayesian
  Particle Filtering.
  \end{abstract}

\section{Introduction}
  Bayesian Particle Filtering (BPF)~\cite{bpf} is an exciting
  new methodology for state space tracking and sensor
  fusion.  The bottleneck step in a typical BPF
  implementation is the weighted resampling step known as
  Sequential Importance Resampling: creating a new
  population of ``particles'' from an old population by
  random sampling from the source population according to
  the particle weights.  Consider resampling $m$ inputs to
  produce $n$ outputs.  The na\"ive algorithm has time
  complexity $O(mn)$.  BPF implementations that resample
  using this expensive $O(mn)$ algorithm can afford to
  resample only sporadically; this represents a difficult
  engineering tradeoff between BPF quality and computational
  cost.

  In this paper we first present an algorithm with
  complexity $O(m + n \log m)$ based on a binary
  heap-structured tree which is straightforward, has good
  performance, and requires no special math.  This is
  essentially a binary search method, often used in faster
  BPF implementations~\cite{arulampalam02tutorial}.  However, we introduce a potential
  performance optimization that takes advantage of the heap
  property.

  Resampling via binary search is often used in faster BPF
  implementations~\cite{arulampalam02tutorial}.  However,
  since the other steps in a BPF iteration are linear in the
  number of particles, a $O(m + n \log m)$ binary search
  will still be the bottleneck step in such an
  implementation---a factor of 10 slowdown for a 1000-particle
  filter, representing significant extra computation.  The
  memory access patterns of a binary search also interact
  badly with the cache of a modern processor.

  We introduce a family of perfect and approximate
  algorithms based on the idea of simulating the scans of
  the na\"ive algorithm in parallel; making a single pass
  through the $m$ input samples and selecting $n$ output
  samples during this pass.

  One well-known method that has been used to regain the
  performance lost to resampling is to give up on
  statistical correctness and simply sample at regular
  intervals~\cite{kitagawa}.  We derive a standard
  approximate $O(m + n)$ algorithms for regular resampling.
  In practice, this seems to work well, and to run quite
  quickly.  However, one cannot help but be a bit concerned
  that regular resampling will ``go wrong'' in a crucial
  situation, due to correlations between the sampling
  interval and an unfortunate particle set.  Although the
  idea does not seem to be widely known, the resistance of
  regular resampling to such correlations can be improved
  simply by shuffling the particles before resampling.
  Since shuffling can be done in $O(n)$ time with good
  constant factors, this is probably a good idea.

  The principal contribution of this work is an algorithm
  that we believe to be novel, with time complexity $O(m +
  n)$ (requiring just the normal $O(m + n)$ space) that
  produces a resampling statistically equivalent to na\"ive
  resampling. For a simple array representation of the
  output, simply initializing the output will require time
  $O(n)$.  It seems that the input must be scanned at least
  once just to determine the total input weight for
  normalization.  Thus, the running time of our algorithms
  is apparently optimal.

  An $O(n)$ algorithm by Beadle and Djuric~\cite{recount}
  produces an output sample that is only approximately
  correctly weighted.  The basic method is to repeatedly
  randomly select an input sample, and then replicate it in
  the output sample the number of times that it would be
  expected to appear based on its fraction of the total
  weight.  As noted above, calculating the total weight
  requires $O(m)$ time, so the advantage of the $O(n)$
  algorithm over an $O(m + n)$ one is essentially lost in
  practice.  The quality of the approximation is shown to be
  reasonably good, and it is shown to work well in one BPF
  application.  However, it is only an approximate method,
  and one might expect that there are limits to the
  approximation quality.  The authors report a speedup of
  approximately 20 for 500 particles, but no other details
  are given about running time.  We suspect that the
  situation is more complicated; to investigate further
  would require independently implementing and timing this
  approach.

  BPF is typically computation-limited, and all of the other
  steps in a BPF iteration require time linear in the
  population size.  By linearizing resampling, we remove the
  resampling bottleneck, allowing higher population
  sizes that in turn dramatically improve BPF performance.

  In the remainder of this paper we review existing
  algorithms, describe our algorithms, and report the
  effectiveness of algorithms in a BPF implementation.  We
  conclude with a discussion of various issues.

\section{Weighted Resampling Algorithms}

  There are a number of approaches to the weighted
  resampling problem.  In this section, we describe some
  weighted resampling algorithms in order of increasing time
  efficiency.  We conclude with the description of some
  $O(m + n)$ algorithms.

  For what follows, assume an array $s$ of $m$ input
  samples, and an output array $s'$ that will hold the $n$
  output samples of the resampling.  Assume further that
  associated with each sample $s_i$ is a weight $w(s_i)$,
  and that the weights have been normalized to sum to 1.
  This can of course be done in time $O(m)$, but typical
  efficient implementations keep a running weight total
  during weight generation, and then normalize their
  sampling range rather than normalizing the weights
  themselves.  We thus discount the normalization cost in
  our analysis.

\subsection{A Na\"ive $O(mn)$ Resampling Algorithm}\label{sec-naive}

  The na\"ive approach to resampling has been re-invented
  many times.  A correct, if inefficient, way to resample is
  via the pseudocode of Figure~\ref{fig-omn}.  The {\em
  sample} procedure selects the first sample such that the
  sum of weights in the input up to and including this
  sample is greater than some index value $\mu$.  The index
  value is chosen in {\em resample} by uniform random
  sampling from the distribution $[0..1]$, with each output
  position being filled in turn.

  \begin{figure}
    \centering
    \begin{minipage}{0.6\textwidth}
      \begin{tabbing}
      XX\=XXXX\=XXXX\=XXXX\=\kill
      {\bf to} {\it sample}$(\mu)$: \\
      \>$t \asgn 0$ \\
      \>{\bf for} $i$ {\bf from} $1$ {\bf to} $m$ {\bf do} \\
      \>\>$t \asgn t + w(s_i)$ \\
      \>\>{\bf if} $t > \mu$ {\bf then} \\
      \>\>\>{\bf return} $s_i$ \newcode
      {\bf to} {\it resample}: \\
      \>{\bf for} $i$ {\bf from} $1$ {\bf to} $n$ {\bf do} \\
      \>\>$\mu \asgn \textit{random-real}([0..1])$ \\
      \>\>$s'_i \asgn sample(\mu)$
      \end{tabbing}
    \end{minipage}
    \caption{Na\"ive Resampling}\label{fig-omn}
  \end{figure}

  The na\"ive algorithm has its advantages.  It is easy to
  verify that it is a perfect sampling algorithm.  It is
  easy to implement, and easy to parallelize.  The expected
  running time is $o(\frac{1}{2}mn)$.  If the distribution
  of weights is uneven enough, as is typical with BPF, the
  proportionality constant can be improved by paying $O(m
  \log m)$ time up front to sort the input array so that the
  largest weights occur first.  Regardless, na\"ive
  resampling is still the bottleneck step in BPF
  implementations that employ it.

\subsection{A Heap-based $O(m + n \log m)$ Resampling Algorithm}\label{sec-heap}

  One simple way to improve the performance of the na\"ive
  algorithm is to improve upon the linear scan performed by
  {\em sample} in Figure~\ref{fig-omn}.

  One way to do this is to treat the input sample array as a
  binary heap.  In time $O(m)$ we can compute and cache the
  sum $w_l$ of weights of the subtree at each position in
  the input, as shown in Figure~\ref{fig-heap}.  The sum at
  each heap position is computed bottom-up and stored as
  $w_t$.

  \begin{figure}
    \centering
    \begin{minipage}{0.6\textwidth}
      \begin{tabbing}
      XX\=XXXX\=XXXX\=XXXX\=\kill
      {\bf to} {\it init-weights}: \\
      \>{\bf for} $i$ {\bf from} $m$ {\bf downto} $1$ {\bf do} \\
      \>\>$l \asgn 2i$ \\
      \>\>$r \asgn 2i + 1$ \\
      \>\>{\bf if} $l > m$ {\bf then} \\
      \>\>\>$w_t(s_i) \asgn w(s_i)$ \\
      \>\>{\bf else if} $r > m$ {\bf then} \\
      \>\>\>$w_t(s_i) \asgn w_t(s_l) + w(s_i)$ \\
      \>\>{\bf else}\\
      \>\>\>$w_t(s_i) \asgn w_t(s_l) + w(s_i) + w_t(s_r)$
    \end{tabbing}
    \end{minipage}
    \caption{Computing Weights of Sub-heaps}\label{fig-heap}
  \end{figure}

  Given $w_t$, {\em sample} can perform a scan for the
  correct input weight in time $O(\log m)$ by scanning down
  from the top of the heap, as shown in
  Figure~\ref{fig-onlm}.  At each step, if the target
  variate $\mu$ is less than the weight of the left subtree,
  the scan descends left.  If $\mu$ is greater than the
  weight of the left subtree by less than the weight of the
  current node the scan terminates and this node is
  selected.  Otherwise, the scan descends right, with
  $\mu$ adjusted downward by the cumulate weight.

  \begin{figure}
    \centering
    \begin{minipage}{0.6\textwidth}
      \begin{tabbing}
      XX\=XXXX\=XXXX\=XXXX\=\kill
      {\bf to} {\it sample}$(\mu,i)$: \\
      \>$l \asgn 2i$ \\
      \>$r \asgn 2i + 1$ \\
      \>{\bf if} $\mu < w_t(s_l)$ {\bf then} \\
      \>\>{\bf return} {\it sample}$(\mu, l)$ \\
      \>{\bf if} $\mu \le w_t(s_l) + w(s_i)$ {\bf then} \\
      \>\>{\bf return} $s_i$ \\
      \>{\bf return} {\it sample}$(\mu - w_t(s_l) - w(s_i), r)$
    \end{tabbing}
    \end{minipage}
    \caption{Heap-based Sampling}\label{fig-onlm}
  \end{figure}

  This algorithm is a bit more complex than the na\"ive one,
  but it dramatically improves upon the worst-case running
  time.

  As with the na\"ive algorithm, a small constant-factor
  improvement is possible by actually heapifying the input
  such that the largest-weighted inputs are near the top.
  Heapification is also $O(m)$ time and can be done in
  place, so there is no computational complexity penalty for
  this optimization.  However, the constant factors must be
  carefully balanced; our experiments show a small net loss
  in some situations and net gain in others.  The
  controlling factor here is the distribution of weights; if
  a few samples carry most of the sample weight,
  heapification will pay for itself.  Since the case in BPF where a
  few samples carry most of the weight tends to be the case where the
  accuracy of the filter is low, this optimization may be of
  special benefit in a variable-population BPF technique
  such as KLD-sampling~\cite{kld}.

  For the normal case of resampling, we would like to get
  rid of the $\log m$ penalty per output sample.  However,
  there is a rarely-occuring special case in which this
  algorithm is especially efficient.  Consider an offline
  sampling problem in which we plan to repeatedly draw a
  small number of samples from the same extremely large
  input distribution.  Because the input distribution
  remains fixed, the cost of heapification can be amortized
  away, yielding an amortized $O(n \log m)$ algorithm.

\subsection{A Merge-based $O(m + n \log n)$ Resampling Algorithm}\label{sec-merge}

  One can imagine trying to improve upon the complexity of
  the heap-style algorithm by using some more efficient data
  structure.  However, there is a fundamental tradeoff---the
  setup for an improved algorithm needs to continue to have
  a cost low in $m$.  Otherwise, any savings in resampling
  will be swamped by setup in the common case that $m
  \approx n$.

  A better plan is to try to improve the na\"ive algorithm
  in a different way.  The real problem with the na\"ive
  algorithm is not so much the cost per scan of the input as it is the
  fact that each scan is independent.  It seems a shame not
  to try to do all the work in one scan.

  Let us generate an array $u$ of $n$ variates up-front,
  then sort it.  At this point, a {\em merge} operation, as
  shown in Figure~\ref{fig-merge}, can be used to generate
  all $n$ outputs in a single pass over the $m$ inputs.  The
  merge operation is simple.  Walk the input array once.
  Each time the sum of weights hits the current variate
  $u_i$, output a sample and move to the next variate
  $u_{i+1}$.  The time complexity of the initial sort is
  $O(n \log n)$ and of the merge pass is $O(m + n)$, for a
  total time complexity of $O(m + n \log n)$.

  \begin{figure}
    \centering
    \begin{minipage}{0.6\textwidth}
      \begin{tabbing}
      XX\=XXXX\=XXXX\=XXXX\=\kill
      {\bf to} {\it merge}$(u)$: \\
      \>$j \asgn 1$ \\
      \>$t \asgn u_1$ \\
      \>{\bf for} $i$ {\bf from} $1$ {\bf to} $n$ {\bf do} \\
      \>\>$\mu \asgn u_i$ \\
      \>\>{\bf while} $\mu < t$ {\bf do} \\
      \>\>\>$t \asgn t + w(s_j)$ \\
      \>\>\>$j \asgn j + 1$ \\
      \>\>$s'_i \asgn s_j$
    \end{tabbing}
    \end{minipage}
    \caption{Merge-based Resampling}\label{fig-merge}
  \end{figure}

  Complexity-wise, we seem to have simply moved the $\log$
  factor of the previous algorithm from $m$ to $n$,
  replacing our $O(m + n \log m)$ algorithm with an $O(m + n
  \log n)$ one.  However, our new algorithm has an important
  distinction.  The log factor this time comes merely from
  sorting an array of uniform variates.  If we could somehow
  generate the variates in sorted order (at amortized
  constant cost) we could make this approach run in time
  $O(m + n)$.  The next section shows how to achieve this.

  Alternatively, if we could sort the variates in $O(n)$
  time, we could get an $O(m + n)$ merge-based algorithm.
  Given that what we are sorting is $n$ variates uniformly
  distributed between $0$ and $1$, a radix sort should do
  the trick here.  However, the constant factors in the running
  time are expected to be poor for large $n$, since the
  radix sort has extremely poor cache behavior.

\subsection{Regular Approximate Resampling}\label{sec-regular}

  For large $m$, a sorted array of uniform deviates looks an
  awful lot like it was produced by sampling at regular
  intervals; the spacing between deviates is pretty uniform.
  This suggests an obvious approximate linear-time
  algorithm, which turns out to be a well-known approach~\cite{kitagawa}:
  sample the $n$ output samples at uniformly-spaced regular
  intervals.  While it is not clear that such a resampling
  algorithm has the same statistical properties as a perfect
  resampling, it seems to be sufficiently good for our BPF
  implementation, and to run about as fast as possible.

  If one is concerned about the possible selection errors of
  regular sampling due to correlation, one might choose to
  shuffle the sample array prior to sampling.  Since the
  shuffle can be performed in $O(m)$ time, there is no
  asymptotic penalty.

\subsection{An Optimal $O(m + n)$ Resampling Algorithm}\label{sec-optimal}

  As discussed in Section~\ref{sec-merge}, if we could generate
  the variates comprising a uniform sample of $n$ values in
  increasing order, we could resample perfectly in time $O(m
  + n)$, improving upon the regular method of Section~\ref{sec-regular}.  In
  this section, we show an approach that achieves this
  goal.

  Assume without loss of generality that our goal is simply
  to generate the first variate in a uniform sample of $n +
  1$ values.  Call the first variate $\mu_0$, the set of
  remaining variates $U$ and note that $|U|=n$.  Now, for
  any given variate $\mu_i \in X$, we have that
    $$ \Prob(\mu_0 < \mu_i) = 1 - \mu_0 $$
  Since this is independently true for each $\mu_i$, we define
    $$p(\mu_0) = \Prob(\forall \mu_i \in U ~.~ \mu_0 < \mu_i) = (1 - \mu_0)^n$$
  
  To see how to take a weighted random sample from this
  distribution, first consider a discrete approximation that
  divides the range $0..1$ into $T$ intervals.  Given a
  random variate $\mu$, we will use $\mu_0=i_0/T$ when
  the sum of the weights of the intervals up to interval $i_0$
  is just greater than $\mu$.  Of course, the weights must be
  normalized by dividing by the total weight.  Thus we have
    $$i_0=\min_{i\in1..T}\left[{\frac{\sum_{j=1}^{i}{p(j/T)}}
                         {\sum_{j=1}^{T}{p(j/T)}} \ge \mu}\right]$$
  Figure~\ref{fig-calculus} illustrates the calculation
  here.  We output as our weighted variate the
  x-coordinate $i_0/T$ of the bar containing $\mu$.

  \begin{figure}
    \centering
    \includegraphics{bars}
    \caption{Discrete Sampling Approximation}\label{fig-calculus}
  \end{figure}

  In the limit as $T\rightarrow\infty$ our discrete
  approximation converges to an integral.  We have
    \begin{eqnarray*}
      \mu &=& \frac{\int_{u=0}^{\mu_0}{(1-u)^n \du}}
                   {\int_{u=0}^{1}{(1-u)^n \du}} \\
	  &=& \frac{\left.\frac{-(1-u)^{n+1}}{n+1}\right|_{u=0}^{\mu_0}}
		   {\left.\frac{-(1-u)^{n+1}}{n+1}\right|_{u=0}^{1}} \\
          &=& \frac{\frac{-1}{n+1}\left[(1-\mu_0)^{n+1}-1\right]}
		   {0-\frac{-1}{n+1}} \\
          &=& 1 - (1 - \mu_0)^{n+1}
    \end{eqnarray*}
  However, what we need is $\mu_0$ in terms of
  $\mu$, so we solve
    \begin{eqnarray*}
       \mu &=& 1 - (1 - \mu_0)^{n+1} \\
       (1 - \mu_0)^{n+1} &=& 1 - \mu \\
       \mu_0 &=& 1 - (1 - \mu)^\frac{1}{n+1} \\
       \mu_0 &=& 1 - \mu^\frac{1}{n+1}
    \end{eqnarray*}
  (The last step is permissible because $\mu$ is a
  uniform deviate in the range $0..1$, and therefore
  statistically equivalent to $(1-\mu)$.)

  We now have the formula we need for selecting the first
  deviate from a set of $n$ in increasing order.  To select
  the next deviate, we simply decrease $n$ by $1$, select a
  deviate from the whole range, and then scale and offset it
  to the remaining range.  We repeat this process until
  $n=0$.  (Recall that $|U|=n$, so the last deviate will be
  selected when $n=0$.)  Figure~\ref{fig-deviate} shows
  this process.
  
  \begin{figure}
    \centering
    \begin{minipage}{0.6\textwidth}
      \begin{tabbing}
      XX\=XXXX\=XXXX\=XXXX\=\kill
      {\bf to} {\it randomize}: \\
      \>$u_1 \asgn (1-\mu)^{\frac{1}{n}}$ \\
      \>{\bf for} $i$ {\bf from} $2$ {\bf to} $n$ {\bf do} \\
      \>\>$u_i \asgn u_{i-1} + (1-u_{i-1})(1-\mu)^{\frac{1}{n-i+1}}$
    \end{tabbing}
    \end{minipage}
    \caption{Generating Deviates In Increasing Order}\label{fig-deviate}
  \end{figure}

  We now have the array $u$ of deviates in sorted order that
  we need to feed the {\em merge} algorithm of the previous
  section.  We thus have an $O(m + n)$ algorithm for random
  weighted selection.

\subsubsection{Faster Generation of Variates}

  The method of directly generating variates described in the
  previous section has a minor limitation: it requires an
  exponentiation per variate.  Even a fast implementation of
  the mathematical function {\tt pow()} on a machine with
  fast floating point is expensive compared to the single
  multiply of regular resampling.

  One possible way around the {\tt pow()} bottleneck is to generate
  random variates with distribution $(1 - x)^n$ by
  Marsaglia and
  Tsang's ``Ziggurat Method'' PRNG~\cite{ziggurat,ziggurat-fixes}.
  Unfortunately, the desired
  distribution is bivariate.  The most direct
  approach would lead to generating $n$ sets of Ziggurat
  tables, which would be prohibitively memory-expensive for
  large $n$.

  When computing $(1 - x)^n$ for large $n$, however, our
  probability expression becomes self-similar, and we can
  accurately approximate the function for larger $n$ using
  the function with smaller $n$.  In fact, this is the
  well-known compound interest problem, yielding an elegant
  limiting approximation.
 $$
\lim_{a \rightarrow \infty}\left(1 - \frac{x}{a}\right)^{an}
  =   \lim_{a \rightarrow \infty}\left(1 + \frac{(-x)}{a}\right)^{an}
  = e^{-xn}$$  This approximation corresponds to a standard 
  linear-time approximate resampling method in which the next sample
  weight is given by an exponentially-distributed
  variate~\cite{carpenter}.  The approximation works well up until near the
  end of the resampling, and is relatively inexpensive if a
  Ziggurat-style exponential generator is used.

  We can get the same performance with perfect resampling,
  though, by modifying a Ziggurat generator for
  $e^{-50\mu_0}$ to accurately compute the desired power by
  tweaking the rejection step.  The efficiency of the
  generator would deteriorate unacceptably in the last 50
  samples, so at that point we just switch to calling {\tt
  pow()} directly.  The resulting resampling implementation
  has performance close to that of regular resampling, but
  with the perfect resampling of the na\"ive method.

  A parallel version of our perfect algorithm is
  straightforward to achieve. Although space precludes a
  detailed description, the basic idea is as follows.  Given
  $p$ processors, each processor generates a variate $v_i$ from the
  distribution $x^i(1-x)^{p-i}$, then binary searches for
  the sample breakpoint corresponding to that variate.  Then
  the processor generates $n/p$ samples in the range
  $v_{i-1}\ldots v_i$ using the perfect resampling
  algorithm.  This achieves a linear speedup while retaining
  statistically correct resampling.

\section{Evaluation}

  We implemented the algorithms described previously in a
  BPF tracker for simulated vehicle navigation.  The
  simulated vehicle has a GPS-like and an IMU-like device
  for navigational purposes; both the device and the vehicle
  model are noisy.  Figures~\ref{fig-track-naive} and
  \ref{fig-track-optimal} show 1000-step actual and estimated vehicle
  tracks from the simulator for the 100-particle case, using
  the na\"ive and optimal algorithms.  Note
  that the quality of tracking of the two algorithms is
  indistinguishable, as expected.

  \begin{figure}
    \begin{minipage}{0.45\linewidth}
    \centering
    \includegraphics[width=0.95\linewidth]{track-naive-100}
    \begin{center}\small (a) Na\"ive Resampling\label{fig-track-naive}\end{center}
    \end{minipage}
    \begin{minipage}{0.45\linewidth}
    \centering
    \includegraphics[width=0.95\linewidth]{track-optimalsort-100}
    \begin{center}\small (b) Optimal Resampling\label{fig-track-optimal}\end{center}
    \end{minipage}
    \caption{Vehicle Tracking Using BPF}
  \end{figure}

  The important distinction between the algorithms we have
  presented is not
  quality, but rather runtime.
  Figures~\ref{fig-times} and~\ref{fig-timeszoom2} show the time
  in seconds for 1000 iterations of BPF with various
  resampling algorithms as a function of the number of
  particles tracked / resampled.  The benchmark machine is
  an otherwise unloaded Intel Core II Duo box at 2.13 GHz
  with 2GB of memory.

  \begin{figure}
    \centering
    \includegraphics{times}
    \caption{Runtimes for BPF Implementation}\label{fig-times}
  \end{figure}

  \begin{figure}
    \centering
    \includegraphics{timeszoom2}
    \caption{Detail of Figure~\ref{fig-times}}\label{fig-timeszoom2}
  \end{figure}

  The distinction between algorithms is somewhat masked by the mediocre running
  time of the rest of the BPF implementation; resampling is
  not the bottleneck for any of these fast algorithms.  The
  performance of the regular algorithm without shuffling illustrates
  this---the cost of that algorithm is quite close to zero,
  and thus represents something of a bound on the
  performance improvement possible through faster resampling.

  As expected, BPF using the na\"ive algorithm becomes
  unusable at larger particle sizes, whereas BPF using the
  optimal algorithm scales linearly.  There does not appear
  to be much advantage in using a heap-based algorithm at
  any particle size.  The Ziggurat implementation of our
  optimal algorithm is quite close to regular resampling
  with shuffle at large particle counts.  Given the
  correctness of our optimal algorithm, we think that this
  represents a significant achievement.

\section{Discussion}

  We have shown an $O(m + n)$ algorithm for perfect
  resampling, and an extremely fast BPF implementation based
  on this algorithm.  However, further speedups are
  possible.  In this section, we discuss some of them.

\subsection{Constant Factors}

  In a typical noise model, there is one call
  to a Gaussian-distributed pseudo-random number generator
  and to the {\tt exp()} function per particle {\em per
  sensor}: this is how the weights are updated.  These are the
  constant-factor bottlenecks in a BPF implementation with
  linear-time or near-linear-time resampling.

  The cost of generating Gaussian pseudo-random numbers can
  be reduced to insignificance by using Marsaglia and
  Tsang's ``Ziggurat Method'' PRNG~\cite{ziggurat}.  We
  observed an approximate doubling of speed in our BPF
  implementation by switching to this PRNG, and it is now
  far from being the bottleneck.

  The {\tt exp()} bottleneck is harder.  We have recently
  switched to a fast approximate exponentiation trick due to
  Schraudolph~\cite{exp}, which improved our performance
  considerably, but the large number of calls is still
  painful.  It may be better to just change noise
  distributions altogether.  Work is still underway in this
  area.


  The remaining bottleneck in resampling itself is the cost
  of the $O(n)$ calls to the {\tt pow()} function, each with
  a slightly different argument.  These rational
  exponentiations are computed during variate generation by
  calling the standard math library function {\tt pow(x,y)}.
  Since there is only one call to {\tt pow()} per particle,
  but many calls to {\tt exp()} per particle, this is not
  the bottleneck step for BPF.  However, it would be nice to
  cut it down, and approaches are available.

\subsubsection{Direct Generation of Variates}

  One way around the {\tt pow()} bottleneck is to generate
  random variates with distribution $(1 - x)^n$ by
  some less expensive method than that of
  Section~\ref{sec-optimal}.  The Ziggurat method mentioned
  previously would be about right, except that the desired
  distribution is a two-argument function.  The most direct
  approach would lead to generating $n$ sets of Ziggurat
  tables, which would be prohibitively memory-expensive for
  large $n$.

  
  When computing $(1 - x)^n$ for large $n$, however, our
  probability expression becomes self-similar, and we can
  accurately approximate the function for larger $n$ using
  the function with smaller $n$. $$
    (1 - x)^n \approx \left(1 - \frac{x}{a}\right)^{an}
  $$
  In fact, this is the well-known compound interest problem,
  yielding an elegant limiting approximation. $$
  \lim_{a \rightarrow \infty}\left(1 - \frac{x}{a}\right)^{an}
  =   \lim_{a \rightarrow \infty}\left(1 + \frac{(-x)}{a}\right)^{an}
  = e^{-xn}$$  This approximation corresponds to a standard 
  linear-time approximate resampling method in which the next sample
  weight is given by an exponentially-distributed
  variate~\cite{carpenter}.  The approximation works well up until near the
  end of the resampling, and is relatively inexpensive if a
  Ziggurat-style exponential generator is used.

  We can do better, though, by modifying the Ziggurat
  generator to accurately compute the desired power by
  tweaking the rejection step---this will converge poorly
  for the last 50 samples or so due to the degenerating
  approximation, at which point we can just switch to
  calling {\tt pow()} directly.

\subsubsection{Segmentation}\label{sec-segment}

  The concentration so far has been on generating the uniform
  variates sequentially.  However, for parallelization one
  would first like to break up the variates into blocks which
  are treated separately.  This is discussed further in
  Section~\ref{sec-parallel}.

  To break the variates up into blocks, we could try to
  derive a direct expression for placing sample $k$ of $n$
  for arbitrary $k$.  This would allow us to ``skip forward''
  by $k$ samples and place a variate, then deal with the
  intervening variates at our leisure.

  We observe that if a uniform variate $\mu_k$ is at
  position $k$ of $n$, $k - 1$ of the samples must have landed to the
  left of $\mu_k$ and $n - k$ of the samples must have
  landed to the right.  There are in general $\tbinom{n}{k-1}$
  ways this can happen, so the probability of $\mu_k$ being
  the $k^{\text{th}}$ sample is $$
    p(\mu_k) = \text{Pr}(\mu_k\text{ is at position }k) =
         \tbinom{n}{k - 1}(\mu_k)^{k-1}(1-\mu_k)^{n - k}
  $$
  The direct method used in Section~\ref{sec-optimal}
  next requires computing the probability that a variate
  $\mu$ is in the right position via $$
    p(\mu) = \text{Pr}(\mu_k = \mu) =
      \frac{\int_{u=0}^{\mu_k}{\tbinom{n}{k - 1}(\mu_k)^k(1-\mu_k)^{n - k}}\du}
           {\int_{u=0}^{1}{\tbinom{n}{k - 1}(\mu_k)^k(1-\mu_k)^{n - k}}\du}
  $$
  The integral in the denominator can be calculated
  directly. $$
    \int_{u=0}^{1}{\tbinom{n}{k - 1}(\mu_k)^k(1-\mu_k)^{n - k}}\du = 
        \tbinom{n}{k - 1}\frac{\Gamma(1 + n - k)\Gamma(k)}{\Gamma(n + 1)}
  $$
  Unfortunately, the integral in the numerator is more of a
  mess: $$
    \int_{u=0}^{\mu_k}{\tbinom{n}{k - 1}(\mu_k)^k(1-\mu_k)^{n - k}}\du =
        \tbinom{n}{k - 1}(\mu_k)^k\,\hyperg(k, -n + k; k + 1; \mu_k)
  $$
  where $\hyperg$ is Gauss's hypergeometric function.

  Any further progress in this direction appears to be
  limited by the difficulty of solving $p(\mu)$
  for $\mu_0$.  A rejection method for direction generation
  of $\mu_0$ would probably be a better approach here---a
  somewhat inefficient method would be fine, since few calls
  would be made to this generator.  Alternatively, one could
  just give up and divide the range uniformly.  The error of
  this approximation should be extremely small, and it would
  avoid a lot of complexity.
  
\subsection{Paralellization}\label{sec-parallel}

  Our optimal algorithm will parallelize reasonably well
  with some additional work. 
  \begin{enumerate}
  \item Each processor fills in a section of total
    accumulated weights in the input particle array as
    described in Section~\ref{sec-merge}.
  \item In a separate pass, each processor adds the sum of
    weights computed by the processor to its left to its
    section of the total accumulated weights in the input
    particle array.
  \item A master processor uses one of the methods of
    ``skipping ahead'' in the variate
    sequence described in the previous section to break up the
    array of variates to be calculated by our $P$-processor
    machine into $P$ regions.
  \item \label{enum-penultimate} Each processor does a search for the start and end
    of its section of the input particle array---the section
    whose total weights contain its variates.  This can be done in time
    $O(\log m)$ using binary search, with an impressively
    small constant factor.
  \item Finally, each processor resamples its section of the array
    using any of the fast algorithms we describe.  The key
    here is that these algorithms are all completely
    parallelizable with zero overhead, given the setup of
    steps~1--\ref{enum-penultimate}.
  \end{enumerate}

\subsection{Recommendations}

  There appear to be three basic regimes that weighted
  resampling is deployed in.  The choice among the algorithms
  presented here may be largely guided by the deployment.

  In the ``offline'' environment, extremely fast desktop
  computers or even supercomputers are being used in
  resampling.  In this regime, the cost of floating point is
  low, the desired resampling accuracy is high, millions or
  billions of input samples may be presented to the
  resampler, and parallel processing is often a real option.
  The perfect optimal algorithm seems like an ideal choice
  in this domain.  One example of this environment is BPF
  for biomedical signal procesing, with which we have had
  some involvement.

  In the ``high performance embedded'' environment, a single
  high-performance CPU, typically without floating point, is
  available, the desired resampling accuracy is high,
  real-time performance is usually required, and thousands
  to tens of thousands of input samples may be preseted to
  the resampler.  In this environment, either our optimal
  resampler or regular resampling with shuffle may be
  appropriate.  One example of this environment is the BPF
  sensor fusion on the PPC Linux flight computer aboard our
  amateur sounding rocket, for which this BPF implementation
  was originally developed.

  In the ``low-end embedded'' environment, a fairly slow
  CPU, typically without floating point, is available, the
  desired resampling accuracy is only moderate, real-time
  performance is usually required, and hundreds to tens of
  thousands of input samples may be presented to the
  resampler.  In this regime, a regular resampler, which
  can be implemented in a very small amount of very fast
  fixed-point or integer code, seems to be the best match.
  One example of this environment is a proprietary real-time
  BPF sensor fusion application for a portable ARM-7 device
  that we are currently working on.

\section*{Availability}

  Our C implementation of BPF with linear resampling
  described here is freely available under the GPL at
  \url{http://wiki.cs.pdx.edu/bartforge/bmpf}.  It relies on
  our BSD-licensed implementation (partly based on the work
  of others---please see the distribution for attribution)
  of various PRNGs and Ziggurat generators at
  \url{http://wiki.cs.pdx.edu/bartforge/ziggurat}.

\section*{Acknowledgments}

  Thanks much to Jules Kongslie, Mark Jones, Dave Archer,
  Jamey Sharp, Josh Triplett and Bryant York for
  illuminating conversations during the discussion of this
  work.  Thanks also to Jules Kongslie and to James McNames
  and his students for patiently explaining BPF to me and
  answering my questions.  Finally, thanks to Keith Packard
  and Intel for providing the hardware on which this work
  was primarily done.

\bibliographystyle{plain}
\bibliography{ltrs}

\end{document}
